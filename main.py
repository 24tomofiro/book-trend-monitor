import os
import yaml
import pandas as pd
from datetime import datetime
import pytz
from dotenv import load_dotenv
from src.crawler import BookCrawler
from src.visualizer import BookVisualizer

def get_time_slot(hour):
    """å®Ÿè¡Œæ™‚åˆ»ã‹ã‚‰æ™‚é–“å¸¯ãƒ©ãƒ™ãƒ«ã‚’è¿”ã™"""
    if 5 <= hour < 11: return "morning"
    elif 11 <= hour < 17: return "afternoon"
    elif 17 <= hour < 23: return "evening"
    else: return "night"

def main():
    print(f"[{datetime.now().isoformat()}] Starting Book Trend Monitor...")

#     # 1. ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿ (.env ã¾ãŸã¯ GitHub Secrets)
#     load_dotenv()
#     api_key = os.environ.get("GOOGLE_API_KEY")
#     cx = os.environ.get("GOOGLE_CX")

#     if not api_key or not cx:
#         print("âŒ Error: GOOGLE_API_KEY or GOOGLE_CX is not set.")
#         return

#     # 2. è¨­å®š(books.yaml)ã®èª­ã¿è¾¼ã¿
#     config_path = "config/books.yaml"
#     if not os.path.exists(config_path):
#         print(f"âŒ Error: {config_path} not found.")
#         return

#     with open(config_path, "r", encoding="utf-8") as f:
#         config = yaml.safe_load(f)
    
#     # 3. å„ç¨®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åˆæœŸåŒ–
#     crawler = BookCrawler(api_key, cx)
#     jst = pytz.timezone('Asia/Tokyo')
#     now = datetime.now(jst)
#     time_slot = get_time_slot(now.hour)
    
#     results = []

#     # 4. åé›†ãƒ«ãƒ¼ãƒ—
#     print(f"ğŸ” Scanning for {len(config['books'])} books (Slot: {time_slot})...")
#     for book in config['books']:
#         title = book['title']
#         # é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã¯ã€æ¤œç´¢ã‚¯ã‚¨ãƒªã«ã€Œ-ãƒ¯ãƒ¼ãƒ‰ã€ã‚’è¿½åŠ 
#         exclude_query = " ".join([f"-{w}" for w in book.get('exclude', [])])
#         keyword = "(" + " OR ".join(book['keywords']) + ") " + exclude_query
        
#         print(f"  - Processing: {title}")
        
#         # Webèª¿æŸ»
#         web_count, web_links, web_sent = crawler.get_data(keyword)
#         # Xèª¿æŸ» (site:x.com é™å®š)
#         x_count, x_links, x_sent = crawler.get_data(keyword, site="x.com")
        
#         results.append({
#             "date": now.strftime("%Y-%m-%d"),
#             "time_slot": time_slot,
#             "book_title": title,
#             "web_count": web_count,
#             "x_count": x_count,
#             "sentiment": round((web_sent + x_sent) / 2, 2),
#             "top_links": ",".join(x_links if x_links else web_links)
#         })

# # 5. CSVä¿å­˜ (ãƒ‡ãƒ¼ã‚¿è“„ç©)
#     df_new = pd.DataFrame(results)
    csv_path = "data/processed/daily_stats.csv"
#     os.makedirs(os.path.dirname(csv_path), exist_ok=True)
    
#     # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€ã‹ã¤ä¸­èº«ãŒç©ºã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
#     if os.path.exists(csv_path) and os.path.getsize(csv_path) > 0:
#         try:
#             df_old = pd.read_csv(csv_path)
#             df_final = pd.concat([df_old, df_new], ignore_index=True)
#             print(f"ğŸ“– Existing data loaded from {csv_path}")
#         except Exception as e:
#             print(f"âš ï¸ Could not read existing CSV ({e}). Creating new one.")
#             df_final = df_new
#     else:
#         # ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„ã€ã¾ãŸã¯ç©ºã®å ´åˆã¯æ–°è¦ä½œæˆ
#         df_final = df_new
#         print(f"ğŸ†• Creating new CSV at {csv_path}")
    
#     # é‡è¤‡å®Ÿè¡Œã‚’é˜²ããŸã‚ã€åŒä¸€æ—¥ãƒ»åŒä¸€æ™‚é–“å¸¯ãƒ»åŒä¸€æ›¸ç±ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°æœ€æ–°ã«æ›´æ–°
#     if not df_final.empty:
#         df_final.drop_duplicates(subset=['date', 'time_slot', 'book_title'], keep='last', inplace=True)
        
#     df_final.to_csv(csv_path, index=False, encoding="utf-8-sig")
#     print(f"âœ… Successfully updated {csv_path}")

# 6. å¯è¦–åŒ–å‡¦ç†
    print("ğŸ“Š Generating charts...")
    visualizer = BookVisualizer(csv_path)
    visualizer.generate_charts()

if __name__ == "__main__":
    main()